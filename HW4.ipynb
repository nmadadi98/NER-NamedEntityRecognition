{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gzip\n",
    "import shutil\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "dataset = datasets.load_dataset(\"conll2003\")\n",
    "\n",
    "train_data = dataset['train']\n",
    "valid_data = dataset['validation']\n",
    "test_data = dataset['test']\n",
    "\n",
    "word_frequency = Counter(itertools.chain(*dataset['train']['tokens']))\n",
    "\n",
    "word_frequency = {\n",
    "    word: frequency\n",
    "    for word, frequency in word_frequency.items()\n",
    "    if frequency >= 3\n",
    "}\n",
    "\n",
    "word2idx = {\n",
    "    word: index\n",
    "    for index, word in enumerate(word_frequency.keys(), start=2)\n",
    "}\n",
    "\n",
    "word2idx['[PAD]'] = 0\n",
    "word2idx['[UNK]'] = 1\n",
    "\n",
    "def convert_word_to_id(sample):\n",
    "    return {\n",
    "        'input_ids': [word2idx.get(token, word2idx['[UNK]']) for token in sample['tokens']],\n",
    "        'labels': sample['ner_tags']\n",
    "\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(convert_word_to_id)\n",
    "\n",
    "for split in dataset.keys():\n",
    "    columns_to_remove = set(dataset[split].column_names) - {'input_ids', 'labels'}\n",
    "    dataset[split] = dataset[split].remove_columns(list(columns_to_remove))\n",
    "\n",
    "X_train = [torch.tensor(s['input_ids']) for s in dataset['train']]\n",
    "y_train = [torch.tensor(s['labels']) for s in dataset['train']]\n",
    "lengths_train = [len(s['input_ids']) for s in dataset['train']]\n",
    "\n",
    "X_valid = [torch.tensor(s['input_ids']) for s in dataset['validation']]\n",
    "y_valid = [torch.tensor(s['labels']) for s in dataset['validation']]\n",
    "lengths_valid = [len(s['input_ids']) for s in dataset['validation']]\n",
    "\n",
    "X_test = [torch.tensor(s['input_ids']) for s in dataset['test']]\n",
    "y_test = [torch.tensor(s['labels']) for s in dataset['test']]\n",
    "lengths_test = [len(s['input_ids']) for s in dataset['test']]\n",
    "\n",
    "X_train_padded = pad_sequence(X_train, batch_first=True, padding_value=word2idx['[PAD]'])\n",
    "y_train_padded = pad_sequence(y_train, batch_first=True, padding_value=9)\n",
    "\n",
    "X_valid_padded = pad_sequence(X_valid, batch_first=True, padding_value=word2idx['[PAD]'])\n",
    "y_valid_padded = pad_sequence(y_valid, batch_first=True, padding_value=9)\n",
    "\n",
    "X_test_padded = pad_sequence(X_test, batch_first=True, padding_value=word2idx['[PAD]'])\n",
    "y_test_padded = pad_sequence(y_test, batch_first=True, padding_value=9)\n",
    "\n",
    "lengths_train = torch.tensor(lengths_train)\n",
    "lengths_valid = torch.tensor(lengths_valid)\n",
    "lengths_test = torch.tensor(lengths_test)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_padded, y_train_padded, lengths_train)\n",
    "valid_dataset = TensorDataset(X_valid_padded, y_valid_padded, lengths_valid)\n",
    "test_dataset = TensorDataset(X_test_padded, y_test_padded, lengths_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8128"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, lstm_hidden_dim, lstm_out_neurons, num_classes):\n",
    "        super().__init__()\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_hidden_dim, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.fc1 = nn.Linear(lstm_hidden_dim*2, lstm_out_neurons)\n",
    "        self.elu = nn.ELU()\n",
    "        self.fc2 = nn.Linear(lstm_out_neurons,num_classes)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "\n",
    "        embed = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embed)\n",
    "        drop_out = self.dropout(lstm_out)\n",
    "        fc1_out = self.fc1(drop_out)\n",
    "        elu_out = self.elu(fc1_out)\n",
    "        fc2_out = self.fc2(elu_out)\n",
    "        \n",
    "        return fc2_out\n",
    "\n",
    "model1 = BLSTM(len(word2idx), 100, 256, 128, 9)\n",
    "criterion1 = nn.CrossEntropyLoss(ignore_index=9)\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.01)\n",
    "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer1,mode='min',patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, dev_dataloader, dev_len, num_epochs, criterion, optimizer, scheduler, saved_model):\n",
    "\n",
    "    min_loss = np.Inf\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for X, y, length in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pack_seq = pack_padded_sequence(X, length, batch_first=True, enforce_sorted=False)\n",
    "            X, _ = pad_packed_sequence(pack_seq, batch_first=True)\n",
    "\n",
    "            output = model(X,length)\n",
    "\n",
    "            y_packed = pack_padded_sequence(y, length, batch_first=True, enforce_sorted=False)\n",
    "            y, _ = pad_packed_sequence(y_packed, batch_first=True)\n",
    "            padding_mask = (y == 0) & (torch.arange(y.size(1))[None, :] >= length[:, None])\n",
    "            y[padding_mask] = 9\n",
    "\n",
    "            loss = criterion(torch.permute(output,(0,2,1)), (y.type(torch.LongTensor)))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        dev_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y, length in dev_dataloader:\n",
    "\n",
    "                pack_seq = pack_padded_sequence(X, length, batch_first=True, enforce_sorted=False)\n",
    "                X, _ = pad_packed_sequence(pack_seq, batch_first=True)\n",
    "\n",
    "                output = model(X, length)\n",
    "\n",
    "                y_packed = pack_padded_sequence(y, length, batch_first=True,enforce_sorted=False)\n",
    "                y, _ = pad_packed_sequence(y_packed, batch_first=True)\n",
    "                padding_mask = (y == 0) & (torch.arange(y.size(1))[None, :] >= length[:, None])\n",
    "                y[padding_mask] = 9\n",
    "                \n",
    "                dev_loss = criterion(torch.permute(output,(0,2,1)), (y.type(torch.LongTensor)))\n",
    "                dev_loss += loss.item()*torch.sum(length)\n",
    "                \n",
    "        scheduler.step(dev_loss)\n",
    "        dev_loss /= dev_len\n",
    "                \n",
    "        if dev_loss <= min_loss:\n",
    "            torch.save(model.state_dict(), saved_model)\n",
    "            min_loss = dev_loss\n",
    "    \n",
    "    model.load_state_dict(torch.load(saved_model))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = train_model(model1, train_dataloader, valid_dataloader, sum(lengths_valid), 30, criterion1, optimizer1, scheduler1, 'model1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('glove.6B.100d.gz', 'rb') as f_in:\n",
    "    with open('glove.6B.100d.txt', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "\n",
    "vocab_glove,embeddings_glove = [],[]\n",
    "with open('glove.6B.100d.txt','rt') as fi:\n",
    "    full_content = fi.read().strip().split('\\n')\n",
    "for i in range(len(full_content)):\n",
    "    i_word = full_content[i].split(' ')[0]\n",
    "    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n",
    "    vocab_glove.append(i_word)\n",
    "    embeddings_glove.append(i_embeddings)\n",
    "\n",
    "vocab_npa = np.array(vocab_glove)\n",
    "embs_npa = np.array(embeddings_glove)\n",
    "\n",
    "vocab_npa = np.insert(vocab_npa, 0, '[PAD]')\n",
    "vocab_npa = np.insert(vocab_npa, 1, '[UNK]')\n",
    "\n",
    "pad_emb_npa = np.zeros((1,embs_npa.shape[1]))\n",
    "unk_emb_npa = np.mean(embs_npa,axis=0,keepdims=True)\n",
    "\n",
    "embs_npa = np.vstack((pad_emb_npa,unk_emb_npa,embs_npa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = vocab_npa.tolist()\n",
    "glove_embedding_features = []\n",
    "\n",
    "for word, i in word2idx.items():\n",
    "    is_title = 1.0 if word.istitle() else 0.0\n",
    "    is_upper = 1.0 if word.isupper() else 0.0\n",
    "    is_lower = 1.0 if word.islower() else 0.0\n",
    "\n",
    "    lower = word.lower()\n",
    "    if lower in vocab_list:\n",
    "        idx = vocab_list.index(lower)\n",
    "        embedding_l = embs_npa[idx]\n",
    "    else:\n",
    "        embedding_l = np.zeros((embs_npa.shape[1]))\n",
    "\n",
    "    embedding_feature = np.concatenate([embedding_l, np.array([is_title, is_upper, is_lower])])\n",
    "    glove_embedding_features.append(embedding_feature)\n",
    "\n",
    "glove_embedding_features = np.array(glove_embedding_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM_Glove(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, lstm_hidden_dim, lstm_out_neurons, num_classes):\n",
    "        super().__init__()\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim=embedding_dim, padding_idx=0).from_pretrained(torch.from_numpy(glove_embedding_features).float(), freeze=False)\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_hidden_dim, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.fc1 = nn.Linear(lstm_hidden_dim*2, lstm_out_neurons)\n",
    "        self.elu = nn.ELU()\n",
    "        self.fc2 = nn.Linear(lstm_out_neurons,num_classes)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "\n",
    "        embed = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embed)\n",
    "        drop_out = self.dropout(lstm_out)\n",
    "        fc1_out = self.fc1(drop_out)\n",
    "        elu_out = self.elu(fc1_out)\n",
    "        fc2_out = self.fc2(elu_out)\n",
    "        \n",
    "        return fc2_out\n",
    "\n",
    "model2 = BLSTM_Glove(len(word2idx), 103, 256, 128, 9)\n",
    "criterion2 = nn.CrossEntropyLoss(ignore_index=9)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
    "scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer2, mode='min', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = train_model(model2, train_dataloader, valid_dataloader, sum(lengths_valid), 30, criterion2, optimizer2, scheduler2, 'model2.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
